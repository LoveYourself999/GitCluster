{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c727fa-600f-4932-9d93-541036603f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import git\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ['OPENAI_API_KEY'] = 'myapikey'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fad40d-6b4b-427c-bc1c-eb69a8931fc3",
   "metadata": {},
   "source": [
    "### Cluster git commits from repo 'pytorch' using k-means and generate a description of each cluster using openAI api with model gpt-3.5-turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cbb8b52-15c9-4fb3-b6e1-56f71c3ac439",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: Summary: This commit message and diff involve reverting a previous commit related to distributed debug handlers in a project.\n",
      "\n",
      "Main Themes:\n",
      "1. Reverting Changes: The primary focus of this commit is to revert a specific commit that was previously made related to distributed debug handlers within the project.\n",
      "2. Build Execution Configuration: There are adjustments made to the build configuration with changes to shared linker flags on Linux and the display of LD flags for different types (Shared, Static, Module).\n",
      "3. Tensor Creation: Some\n",
      "\n",
      "Cluster 1: **Themes and Topics:**\n",
      "\n",
      "1. **New Features in CommDebugMode:**\n",
      "   - The primary theme across both commits is the enhancement of CommDebugMode functionality with the addition of new tracing features - `c10d alltoall_` and `c10d alltoall_base_`.\n",
      "   - This indicates a focus on improving debugging and monitoring capabilities within the distributed communication module.\n",
      "\n",
      "2. **Integration of WorkerServer:**\n",
      "   - Both commits introduce the `WorkerServer` class,\n",
      "\n",
      "Cluster 2: The main theme of this commit message and diff is related to optimizing linker script for CUDA ARM wheel in a CMAKE rule. The diff would have changes to the CMakeLists.txt file to include the optimization flag for linker script. It also references the original pull request and lists the approvers of this commit.\n",
      "\n",
      "This commit focuses on improving the performance of the linker script for CUDA ARM wheel by adding an optimization flag to the CMAKE rule. The diff would show changes made to the CMakeLists.txt\n",
      "\n",
      "Cluster 3: Summary:\n",
      "The commit message describes a reversion of a bugfix related to the nn parameter construction in the \"dynamo\" module. The diff contains changes related to the shared linker flags in the project.\n",
      "\n",
      "Main Themes and Topics:\n",
      "1. Reverting a Bugfix: The commit message highlights the reversion of a bugfix commit that was causing failures in nn tests in the dynamo module. The reason for reverting is mentioned as the bugfix leading to test failures.\n",
      "\n",
      "2. Shared Linker\n",
      "\n",
      "Cluster 4: Summary:\n",
      "1. Revert the changes made to enable subprocess-based parallel compile as the default due to flaky inductor failures.\n",
      "2. Bugfix for nn parameter construction in dynamo.\n",
      "\n",
      "Themes and Topics:\n",
      "1. Reverting Changes: The first commit message and diff mention a revert of changes that enabled subprocess-based parallel compile as the default due to issues with flaky inductor failures.\n",
      "2. Bugfix in Dynamo: The second commit message and diff indicate a bugfix related to nn parameter construction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def describe_clusters(commit_messages, labels, num_clusters):\n",
    "    cluster_descriptions = []\n",
    "\n",
    "    for cluster_id in range(num_clusters):\n",
    "        # Get all commit messages for this cluster\n",
    "        cluster_messages = [commit_messages[i] for i in range(len(labels)) if labels[i] == cluster_id]\n",
    "        \n",
    "        # Use only a subset if there are too many messages\n",
    "        if len(cluster_messages) > 10:\n",
    "            cluster_messages = cluster_messages[:10]\n",
    "        \n",
    "        prompt = \"Here are some commit messages and diffs from a project. First, give a short summaries and then provide details of the main themes and topics described in these messages and diffs:\\n\\n\"\n",
    "        prompt += \"\\n\\n\".join(cluster_messages)\n",
    "        \n",
    "        client = OpenAI(\n",
    "        # This is the default and can be omitted\n",
    "            api_key=openai.api_key\n",
    "        )\n",
    "        response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        }\n",
    "        ],\n",
    "        max_tokens=100\n",
    "        )\n",
    "        description = response.choices[0].message.content\n",
    "        cluster_descriptions.append(\"Cluster {}: {}\".format(cluster_id, description) + \"\\n\")\n",
    "    \n",
    "    return cluster_descriptions\n",
    "\n",
    "repo_path = '~/Downloads/pytorch'  # Update this path to your cloned repo\n",
    "repo = git.Repo(repo_path)\n",
    "\n",
    "# Extract commit messages and diffs\n",
    "commits = list(repo.iter_commits('main', max_count=7))\n",
    "commit_contents = []\n",
    "\n",
    "for commit in commits:\n",
    "    commit_message = commit.message\n",
    "    commit_diff = commit.diff(create_patch=True)\n",
    "\n",
    "    diff_texts = []\n",
    "    for diff in commit_diff:\n",
    "        diff_texts.append(diff.diff.decode('utf-8'))\n",
    "\n",
    "    commit_diff_text = '\\n'.join(diff_texts)\n",
    "    content = f\"Commit Message:\\n{commit_message}\\n\\nCommit Diff:\\n{commit_diff_text}\"\n",
    "    commit_contents.append(content)\n",
    "\n",
    "# Check if commit contents are not empty\n",
    "if not commit_contents:\n",
    "    raise ValueError(\"No commit contents found in the repository\")\n",
    "\n",
    "# Vectorize commit contents using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(commit_contents)\n",
    "\n",
    "# Perform k-means clustering\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get cluster descriptions\n",
    "cluster_descriptions = describe_clusters(commit_contents, labels, num_clusters)\n",
    "\n",
    "# Print cluster descriptions\n",
    "for description in cluster_descriptions:\n",
    "    print(description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2513acb-7933-4bce-a053-09821ad72c30",
   "metadata": {},
   "source": [
    "### Here I list all the commit message for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ef54181-ebc9-46ad-b57a-19f1bc454090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: Commit messages: Commit----------------------------------\n",
      "Add linker script optimization flag to CMAKE rule for CUDA ARM wheel (#127514)\n",
      "\n",
      "Original PR - https://github.com/pytorch/pytorch/pull/127220\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127514\n",
      "Approved by: https://github.com/Aidyn-A, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Revert \"[dynamo] Bugfix for nn parameter construction (#127806)\"\n",
      "\n",
      "This reverts commit f27c4dd862bf79f37019ef277957cd577d57b66f.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127806 on behalf of https://github.com/PaliC due to causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))\n",
      ", Commit----------------------------------\n",
      "Reapply \"distributed debug handlers (#126601)\" (#127805)\n",
      "\n",
      "This reverts commit 7646825c3eb687030c4f873b01312be0eed80174.\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127805\n",
      "Approved by: https://github.com/PaliC\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode (#127360)\n",
      "\n",
      "**Summary**\n",
      "Added c10d alltoall_ and alltoall_base tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127360\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      "ghstack dependencies: #127358\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode (#127358)\n",
      "\n",
      "**Summary**\n",
      "Added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127358\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      ", Commit----------------------------------\n",
      "Revert \"[inductor] Enable subprocess-based parallel compile as the default (#126817)\"\n",
      "\n",
      "This reverts commit cf77e7dd9770caf65e898ac2ee82045aa0408e30.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/126817 on behalf of https://github.com/huydhn due to There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))\n",
      ", Commit----------------------------------\n",
      "[dynamo] Bugfix for nn parameter construction (#127806)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127806\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785, #127802\n",
      ", Commit----------------------------------\n",
      "[dynamo] Unspec nn module when global backward hooks are present (#127802)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127802\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785\n",
      ", Commit----------------------------------\n",
      "[dynamo] Tensorvariable - track grad with _grad field (#127785)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127785\n",
      "Approved by: https://github.com/jansel\n",
      ", Commit----------------------------------\n",
      "Using scalarType instead string in function _group_tensors_by_device_and_dtype. (#127869)\n",
      "\n",
      "Now torch.dtype can pass through pybind11, so modify function _group_tensors_by_device_and_dtype to using scalar type. And without convert torch.dtype and string in python and c++ side.\n",
      "@ezyang @bdhirsh\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127869\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Revert \"Retire torch.distributed.pipeline (#127354)\"\n",
      "\n",
      "This reverts commit b9c058c203ee38032594f898f27cd8404f113a63.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127354 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))\n",
      ", Commit----------------------------------\n",
      "[CI] disable td for xpu ci test by default (#127611)\n",
      "\n",
      "Due to the xpu ci test has been enabled td by default, a lot of test cases (75%) have been skipped in CI tests. It caused some ci failures escaped from the ci tests, for example issue #127539. This PR depends on PR #127595 landed.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127611\n",
      "Approved by: https://github.com/etaf, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Enable UFMT on test/test_jit_fuser_te.py (#127759)\n",
      "\n",
      "Part of #123062\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127759\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Use freshly traced jit-traced module to be used in export analysis (#127577)\n",
      "\n",
      "Summary: When we export already traced module, it seems to be modifying some global state causing the traced modules to fail to run. For now, we are only logging for test cases, so it is probs ok to trace fresh copy to be used in export for now.\n",
      "\n",
      "Test Plan: CI\n",
      "\n",
      "Differential Revision: D57983518\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127577\n",
      "Approved by: https://github.com/pianpwk\n",
      ", Commit----------------------------------\n",
      "[BE]: Update cudnn to 9.1.0.70 (#123475)\n",
      "\n",
      "cuDNN has managed to upload cu11 and cu12 wheels for ~~9.0.0.312~~ 9.1.0.70, so trying this out...\n",
      "\n",
      "CC @Skylion007 @malfet\n",
      "\n",
      "Co-authored-by: Wei Wang <weiwan@nvidia.com>\n",
      "Co-authored-by: atalman <atalman@fb.com>\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/123475\n",
      "Approved by: https://github.com/Skylion007, https://github.com/malfet, https://github.com/nWEIdia\n",
      ", Commit----------------------------------\n",
      "documentation for pattern_matcher.py (#127459)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127459\n",
      "Approved by: https://github.com/oulgen\n",
      "ghstack dependencies: #127457, #127458\n",
      ", Commit----------------------------------\n",
      "Add typing annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Turn on `mypy: disallow-untyped-defs` in pattern_matcher.py and fix the fallout.\n",
      "\n",
      "There are still a bunch of `type: ignore` annotations which should eventually be ironed out.\n",
      "\n",
      "In the processs found a bug: #127457\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127458\n",
      "Approved by: https://github.com/Skylion007\n",
      "ghstack dependencies: #127457\n",
      ", Commit----------------------------------\n",
      "fix post_grad pattern (#127457)\n",
      "\n",
      "The lowering pattern built by cuda_and_enabled_mixed_mm_and_not_int8() was using ListOf() incorrectly - ListOf() is meant to represent a single repeating pattern - but cuda_and_enabled_mixed_mm_and_not_int8() was passing two patterns - I think based on the comment it's trying to build a sequence which would be represented by an actual list, not ListOf().\n",
      "\n",
      "The behavior of the existing pattern would be to pass the second pattern as the `partial` parameter of `ListOf` which is meant to be a boolean - so it's almost certainly not what was intended.\n",
      "\n",
      "I tried changing it to be what I thought was the intended behavior but then the resnet152 test failed accuracy - so I'm just preserving the existing behavior with the correct parameter types.\n",
      "\n",
      "Found when adding annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127457\n",
      "Approved by: https://github.com/oulgen\n",
      ", Commit----------------------------------\n",
      "Concat namespaces and other fixes in torch/csrc/utils (#127833)\n",
      "\n",
      "It contains formatting and other minor fixes.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127833\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n",
      "\n",
      "There is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n",
      "\n",
      "```\n",
      "at::Tensor temp_q = q;\n",
      "if (seqlenq_ngroups_swapped) {\n",
      "        temp_q = q.reshape( ...\n",
      " ...\n",
      "}\n",
      "const int total_q = q.sizes()[0];\n",
      "CHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n",
      "```\n",
      "\n",
      "When doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n",
      "```\n",
      "const int total_q = temp_q.sizes()[0];\n",
      " ```\n",
      "\n",
      "In the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127524\n",
      "Approved by: https://github.com/drisspg\n",
      "\n",
      "Cluster 1: Commit messages: Commit----------------------------------\n",
      "Add linker script optimization flag to CMAKE rule for CUDA ARM wheel (#127514)\n",
      "\n",
      "Original PR - https://github.com/pytorch/pytorch/pull/127220\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127514\n",
      "Approved by: https://github.com/Aidyn-A, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Revert \"[dynamo] Bugfix for nn parameter construction (#127806)\"\n",
      "\n",
      "This reverts commit f27c4dd862bf79f37019ef277957cd577d57b66f.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127806 on behalf of https://github.com/PaliC due to causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))\n",
      ", Commit----------------------------------\n",
      "Reapply \"distributed debug handlers (#126601)\" (#127805)\n",
      "\n",
      "This reverts commit 7646825c3eb687030c4f873b01312be0eed80174.\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127805\n",
      "Approved by: https://github.com/PaliC\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode (#127360)\n",
      "\n",
      "**Summary**\n",
      "Added c10d alltoall_ and alltoall_base tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127360\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      "ghstack dependencies: #127358\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode (#127358)\n",
      "\n",
      "**Summary**\n",
      "Added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127358\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      ", Commit----------------------------------\n",
      "Revert \"[inductor] Enable subprocess-based parallel compile as the default (#126817)\"\n",
      "\n",
      "This reverts commit cf77e7dd9770caf65e898ac2ee82045aa0408e30.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/126817 on behalf of https://github.com/huydhn due to There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))\n",
      ", Commit----------------------------------\n",
      "[dynamo] Bugfix for nn parameter construction (#127806)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127806\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785, #127802\n",
      ", Commit----------------------------------\n",
      "[dynamo] Unspec nn module when global backward hooks are present (#127802)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127802\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785\n",
      ", Commit----------------------------------\n",
      "[dynamo] Tensorvariable - track grad with _grad field (#127785)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127785\n",
      "Approved by: https://github.com/jansel\n",
      ", Commit----------------------------------\n",
      "Using scalarType instead string in function _group_tensors_by_device_and_dtype. (#127869)\n",
      "\n",
      "Now torch.dtype can pass through pybind11, so modify function _group_tensors_by_device_and_dtype to using scalar type. And without convert torch.dtype and string in python and c++ side.\n",
      "@ezyang @bdhirsh\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127869\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Revert \"Retire torch.distributed.pipeline (#127354)\"\n",
      "\n",
      "This reverts commit b9c058c203ee38032594f898f27cd8404f113a63.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127354 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))\n",
      ", Commit----------------------------------\n",
      "[CI] disable td for xpu ci test by default (#127611)\n",
      "\n",
      "Due to the xpu ci test has been enabled td by default, a lot of test cases (75%) have been skipped in CI tests. It caused some ci failures escaped from the ci tests, for example issue #127539. This PR depends on PR #127595 landed.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127611\n",
      "Approved by: https://github.com/etaf, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Enable UFMT on test/test_jit_fuser_te.py (#127759)\n",
      "\n",
      "Part of #123062\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127759\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Use freshly traced jit-traced module to be used in export analysis (#127577)\n",
      "\n",
      "Summary: When we export already traced module, it seems to be modifying some global state causing the traced modules to fail to run. For now, we are only logging for test cases, so it is probs ok to trace fresh copy to be used in export for now.\n",
      "\n",
      "Test Plan: CI\n",
      "\n",
      "Differential Revision: D57983518\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127577\n",
      "Approved by: https://github.com/pianpwk\n",
      ", Commit----------------------------------\n",
      "[BE]: Update cudnn to 9.1.0.70 (#123475)\n",
      "\n",
      "cuDNN has managed to upload cu11 and cu12 wheels for ~~9.0.0.312~~ 9.1.0.70, so trying this out...\n",
      "\n",
      "CC @Skylion007 @malfet\n",
      "\n",
      "Co-authored-by: Wei Wang <weiwan@nvidia.com>\n",
      "Co-authored-by: atalman <atalman@fb.com>\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/123475\n",
      "Approved by: https://github.com/Skylion007, https://github.com/malfet, https://github.com/nWEIdia\n",
      ", Commit----------------------------------\n",
      "documentation for pattern_matcher.py (#127459)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127459\n",
      "Approved by: https://github.com/oulgen\n",
      "ghstack dependencies: #127457, #127458\n",
      ", Commit----------------------------------\n",
      "Add typing annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Turn on `mypy: disallow-untyped-defs` in pattern_matcher.py and fix the fallout.\n",
      "\n",
      "There are still a bunch of `type: ignore` annotations which should eventually be ironed out.\n",
      "\n",
      "In the processs found a bug: #127457\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127458\n",
      "Approved by: https://github.com/Skylion007\n",
      "ghstack dependencies: #127457\n",
      ", Commit----------------------------------\n",
      "fix post_grad pattern (#127457)\n",
      "\n",
      "The lowering pattern built by cuda_and_enabled_mixed_mm_and_not_int8() was using ListOf() incorrectly - ListOf() is meant to represent a single repeating pattern - but cuda_and_enabled_mixed_mm_and_not_int8() was passing two patterns - I think based on the comment it's trying to build a sequence which would be represented by an actual list, not ListOf().\n",
      "\n",
      "The behavior of the existing pattern would be to pass the second pattern as the `partial` parameter of `ListOf` which is meant to be a boolean - so it's almost certainly not what was intended.\n",
      "\n",
      "I tried changing it to be what I thought was the intended behavior but then the resnet152 test failed accuracy - so I'm just preserving the existing behavior with the correct parameter types.\n",
      "\n",
      "Found when adding annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127457\n",
      "Approved by: https://github.com/oulgen\n",
      ", Commit----------------------------------\n",
      "Concat namespaces and other fixes in torch/csrc/utils (#127833)\n",
      "\n",
      "It contains formatting and other minor fixes.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127833\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n",
      "\n",
      "There is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n",
      "\n",
      "```\n",
      "at::Tensor temp_q = q;\n",
      "if (seqlenq_ngroups_swapped) {\n",
      "        temp_q = q.reshape( ...\n",
      " ...\n",
      "}\n",
      "const int total_q = q.sizes()[0];\n",
      "CHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n",
      "```\n",
      "\n",
      "When doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n",
      "```\n",
      "const int total_q = temp_q.sizes()[0];\n",
      " ```\n",
      "\n",
      "In the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127524\n",
      "Approved by: https://github.com/drisspg\n",
      "\n",
      "Cluster 2: Commit messages: Commit----------------------------------\n",
      "Add linker script optimization flag to CMAKE rule for CUDA ARM wheel (#127514)\n",
      "\n",
      "Original PR - https://github.com/pytorch/pytorch/pull/127220\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127514\n",
      "Approved by: https://github.com/Aidyn-A, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Revert \"[dynamo] Bugfix for nn parameter construction (#127806)\"\n",
      "\n",
      "This reverts commit f27c4dd862bf79f37019ef277957cd577d57b66f.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127806 on behalf of https://github.com/PaliC due to causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))\n",
      ", Commit----------------------------------\n",
      "Reapply \"distributed debug handlers (#126601)\" (#127805)\n",
      "\n",
      "This reverts commit 7646825c3eb687030c4f873b01312be0eed80174.\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127805\n",
      "Approved by: https://github.com/PaliC\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode (#127360)\n",
      "\n",
      "**Summary**\n",
      "Added c10d alltoall_ and alltoall_base tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127360\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      "ghstack dependencies: #127358\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode (#127358)\n",
      "\n",
      "**Summary**\n",
      "Added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127358\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      ", Commit----------------------------------\n",
      "Revert \"[inductor] Enable subprocess-based parallel compile as the default (#126817)\"\n",
      "\n",
      "This reverts commit cf77e7dd9770caf65e898ac2ee82045aa0408e30.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/126817 on behalf of https://github.com/huydhn due to There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))\n",
      ", Commit----------------------------------\n",
      "[dynamo] Bugfix for nn parameter construction (#127806)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127806\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785, #127802\n",
      ", Commit----------------------------------\n",
      "[dynamo] Unspec nn module when global backward hooks are present (#127802)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127802\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785\n",
      ", Commit----------------------------------\n",
      "[dynamo] Tensorvariable - track grad with _grad field (#127785)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127785\n",
      "Approved by: https://github.com/jansel\n",
      ", Commit----------------------------------\n",
      "Using scalarType instead string in function _group_tensors_by_device_and_dtype. (#127869)\n",
      "\n",
      "Now torch.dtype can pass through pybind11, so modify function _group_tensors_by_device_and_dtype to using scalar type. And without convert torch.dtype and string in python and c++ side.\n",
      "@ezyang @bdhirsh\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127869\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Revert \"Retire torch.distributed.pipeline (#127354)\"\n",
      "\n",
      "This reverts commit b9c058c203ee38032594f898f27cd8404f113a63.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127354 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))\n",
      ", Commit----------------------------------\n",
      "[CI] disable td for xpu ci test by default (#127611)\n",
      "\n",
      "Due to the xpu ci test has been enabled td by default, a lot of test cases (75%) have been skipped in CI tests. It caused some ci failures escaped from the ci tests, for example issue #127539. This PR depends on PR #127595 landed.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127611\n",
      "Approved by: https://github.com/etaf, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Enable UFMT on test/test_jit_fuser_te.py (#127759)\n",
      "\n",
      "Part of #123062\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127759\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Use freshly traced jit-traced module to be used in export analysis (#127577)\n",
      "\n",
      "Summary: When we export already traced module, it seems to be modifying some global state causing the traced modules to fail to run. For now, we are only logging for test cases, so it is probs ok to trace fresh copy to be used in export for now.\n",
      "\n",
      "Test Plan: CI\n",
      "\n",
      "Differential Revision: D57983518\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127577\n",
      "Approved by: https://github.com/pianpwk\n",
      ", Commit----------------------------------\n",
      "[BE]: Update cudnn to 9.1.0.70 (#123475)\n",
      "\n",
      "cuDNN has managed to upload cu11 and cu12 wheels for ~~9.0.0.312~~ 9.1.0.70, so trying this out...\n",
      "\n",
      "CC @Skylion007 @malfet\n",
      "\n",
      "Co-authored-by: Wei Wang <weiwan@nvidia.com>\n",
      "Co-authored-by: atalman <atalman@fb.com>\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/123475\n",
      "Approved by: https://github.com/Skylion007, https://github.com/malfet, https://github.com/nWEIdia\n",
      ", Commit----------------------------------\n",
      "documentation for pattern_matcher.py (#127459)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127459\n",
      "Approved by: https://github.com/oulgen\n",
      "ghstack dependencies: #127457, #127458\n",
      ", Commit----------------------------------\n",
      "Add typing annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Turn on `mypy: disallow-untyped-defs` in pattern_matcher.py and fix the fallout.\n",
      "\n",
      "There are still a bunch of `type: ignore` annotations which should eventually be ironed out.\n",
      "\n",
      "In the processs found a bug: #127457\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127458\n",
      "Approved by: https://github.com/Skylion007\n",
      "ghstack dependencies: #127457\n",
      ", Commit----------------------------------\n",
      "fix post_grad pattern (#127457)\n",
      "\n",
      "The lowering pattern built by cuda_and_enabled_mixed_mm_and_not_int8() was using ListOf() incorrectly - ListOf() is meant to represent a single repeating pattern - but cuda_and_enabled_mixed_mm_and_not_int8() was passing two patterns - I think based on the comment it's trying to build a sequence which would be represented by an actual list, not ListOf().\n",
      "\n",
      "The behavior of the existing pattern would be to pass the second pattern as the `partial` parameter of `ListOf` which is meant to be a boolean - so it's almost certainly not what was intended.\n",
      "\n",
      "I tried changing it to be what I thought was the intended behavior but then the resnet152 test failed accuracy - so I'm just preserving the existing behavior with the correct parameter types.\n",
      "\n",
      "Found when adding annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127457\n",
      "Approved by: https://github.com/oulgen\n",
      ", Commit----------------------------------\n",
      "Concat namespaces and other fixes in torch/csrc/utils (#127833)\n",
      "\n",
      "It contains formatting and other minor fixes.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127833\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n",
      "\n",
      "There is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n",
      "\n",
      "```\n",
      "at::Tensor temp_q = q;\n",
      "if (seqlenq_ngroups_swapped) {\n",
      "        temp_q = q.reshape( ...\n",
      " ...\n",
      "}\n",
      "const int total_q = q.sizes()[0];\n",
      "CHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n",
      "```\n",
      "\n",
      "When doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n",
      "```\n",
      "const int total_q = temp_q.sizes()[0];\n",
      " ```\n",
      "\n",
      "In the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127524\n",
      "Approved by: https://github.com/drisspg\n",
      "\n",
      "Cluster 3: Commit messages: Commit----------------------------------\n",
      "Add linker script optimization flag to CMAKE rule for CUDA ARM wheel (#127514)\n",
      "\n",
      "Original PR - https://github.com/pytorch/pytorch/pull/127220\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127514\n",
      "Approved by: https://github.com/Aidyn-A, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Revert \"[dynamo] Bugfix for nn parameter construction (#127806)\"\n",
      "\n",
      "This reverts commit f27c4dd862bf79f37019ef277957cd577d57b66f.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127806 on behalf of https://github.com/PaliC due to causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))\n",
      ", Commit----------------------------------\n",
      "Reapply \"distributed debug handlers (#126601)\" (#127805)\n",
      "\n",
      "This reverts commit 7646825c3eb687030c4f873b01312be0eed80174.\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127805\n",
      "Approved by: https://github.com/PaliC\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode (#127360)\n",
      "\n",
      "**Summary**\n",
      "Added c10d alltoall_ and alltoall_base tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127360\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      "ghstack dependencies: #127358\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode (#127358)\n",
      "\n",
      "**Summary**\n",
      "Added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127358\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      ", Commit----------------------------------\n",
      "Revert \"[inductor] Enable subprocess-based parallel compile as the default (#126817)\"\n",
      "\n",
      "This reverts commit cf77e7dd9770caf65e898ac2ee82045aa0408e30.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/126817 on behalf of https://github.com/huydhn due to There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))\n",
      ", Commit----------------------------------\n",
      "[dynamo] Bugfix for nn parameter construction (#127806)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127806\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785, #127802\n",
      ", Commit----------------------------------\n",
      "[dynamo] Unspec nn module when global backward hooks are present (#127802)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127802\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785\n",
      ", Commit----------------------------------\n",
      "[dynamo] Tensorvariable - track grad with _grad field (#127785)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127785\n",
      "Approved by: https://github.com/jansel\n",
      ", Commit----------------------------------\n",
      "Using scalarType instead string in function _group_tensors_by_device_and_dtype. (#127869)\n",
      "\n",
      "Now torch.dtype can pass through pybind11, so modify function _group_tensors_by_device_and_dtype to using scalar type. And without convert torch.dtype and string in python and c++ side.\n",
      "@ezyang @bdhirsh\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127869\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Revert \"Retire torch.distributed.pipeline (#127354)\"\n",
      "\n",
      "This reverts commit b9c058c203ee38032594f898f27cd8404f113a63.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127354 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))\n",
      ", Commit----------------------------------\n",
      "[CI] disable td for xpu ci test by default (#127611)\n",
      "\n",
      "Due to the xpu ci test has been enabled td by default, a lot of test cases (75%) have been skipped in CI tests. It caused some ci failures escaped from the ci tests, for example issue #127539. This PR depends on PR #127595 landed.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127611\n",
      "Approved by: https://github.com/etaf, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Enable UFMT on test/test_jit_fuser_te.py (#127759)\n",
      "\n",
      "Part of #123062\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127759\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Use freshly traced jit-traced module to be used in export analysis (#127577)\n",
      "\n",
      "Summary: When we export already traced module, it seems to be modifying some global state causing the traced modules to fail to run. For now, we are only logging for test cases, so it is probs ok to trace fresh copy to be used in export for now.\n",
      "\n",
      "Test Plan: CI\n",
      "\n",
      "Differential Revision: D57983518\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127577\n",
      "Approved by: https://github.com/pianpwk\n",
      ", Commit----------------------------------\n",
      "[BE]: Update cudnn to 9.1.0.70 (#123475)\n",
      "\n",
      "cuDNN has managed to upload cu11 and cu12 wheels for ~~9.0.0.312~~ 9.1.0.70, so trying this out...\n",
      "\n",
      "CC @Skylion007 @malfet\n",
      "\n",
      "Co-authored-by: Wei Wang <weiwan@nvidia.com>\n",
      "Co-authored-by: atalman <atalman@fb.com>\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/123475\n",
      "Approved by: https://github.com/Skylion007, https://github.com/malfet, https://github.com/nWEIdia\n",
      ", Commit----------------------------------\n",
      "documentation for pattern_matcher.py (#127459)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127459\n",
      "Approved by: https://github.com/oulgen\n",
      "ghstack dependencies: #127457, #127458\n",
      ", Commit----------------------------------\n",
      "Add typing annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Turn on `mypy: disallow-untyped-defs` in pattern_matcher.py and fix the fallout.\n",
      "\n",
      "There are still a bunch of `type: ignore` annotations which should eventually be ironed out.\n",
      "\n",
      "In the processs found a bug: #127457\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127458\n",
      "Approved by: https://github.com/Skylion007\n",
      "ghstack dependencies: #127457\n",
      ", Commit----------------------------------\n",
      "fix post_grad pattern (#127457)\n",
      "\n",
      "The lowering pattern built by cuda_and_enabled_mixed_mm_and_not_int8() was using ListOf() incorrectly - ListOf() is meant to represent a single repeating pattern - but cuda_and_enabled_mixed_mm_and_not_int8() was passing two patterns - I think based on the comment it's trying to build a sequence which would be represented by an actual list, not ListOf().\n",
      "\n",
      "The behavior of the existing pattern would be to pass the second pattern as the `partial` parameter of `ListOf` which is meant to be a boolean - so it's almost certainly not what was intended.\n",
      "\n",
      "I tried changing it to be what I thought was the intended behavior but then the resnet152 test failed accuracy - so I'm just preserving the existing behavior with the correct parameter types.\n",
      "\n",
      "Found when adding annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127457\n",
      "Approved by: https://github.com/oulgen\n",
      ", Commit----------------------------------\n",
      "Concat namespaces and other fixes in torch/csrc/utils (#127833)\n",
      "\n",
      "It contains formatting and other minor fixes.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127833\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n",
      "\n",
      "There is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n",
      "\n",
      "```\n",
      "at::Tensor temp_q = q;\n",
      "if (seqlenq_ngroups_swapped) {\n",
      "        temp_q = q.reshape( ...\n",
      " ...\n",
      "}\n",
      "const int total_q = q.sizes()[0];\n",
      "CHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n",
      "```\n",
      "\n",
      "When doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n",
      "```\n",
      "const int total_q = temp_q.sizes()[0];\n",
      " ```\n",
      "\n",
      "In the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127524\n",
      "Approved by: https://github.com/drisspg\n",
      "\n",
      "Cluster 4: Commit messages: Commit----------------------------------\n",
      "Add linker script optimization flag to CMAKE rule for CUDA ARM wheel (#127514)\n",
      "\n",
      "Original PR - https://github.com/pytorch/pytorch/pull/127220\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127514\n",
      "Approved by: https://github.com/Aidyn-A, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Revert \"[dynamo] Bugfix for nn parameter construction (#127806)\"\n",
      "\n",
      "This reverts commit f27c4dd862bf79f37019ef277957cd577d57b66f.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127806 on behalf of https://github.com/PaliC due to causing nn tests to fail ([comment](https://github.com/pytorch/pytorch/pull/127806#issuecomment-2148393903))\n",
      ", Commit----------------------------------\n",
      "Reapply \"distributed debug handlers (#126601)\" (#127805)\n",
      "\n",
      "This reverts commit 7646825c3eb687030c4f873b01312be0eed80174.\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127805\n",
      "Approved by: https://github.com/PaliC\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d alltoall_ and alltoall_base_ to CommDebugMode (#127360)\n",
      "\n",
      "**Summary**\n",
      "Added c10d alltoall_ and alltoall_base tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127360\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      "ghstack dependencies: #127358\n",
      ", Commit----------------------------------\n",
      "[dtensor][debug] added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing_ to CommDebugMode (#127358)\n",
      "\n",
      "**Summary**\n",
      "Added c10d reduce_scatter_ and reduce_scatter_tensor_coalesced tracing to CommDebugMode and edited test case in test_comm_mode to include added features.\n",
      "\n",
      "**Test Plan**\n",
      "pytest test/distributed/_tensor/debug/test_comm_mode.py\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127358\n",
      "Approved by: https://github.com/wz337, https://github.com/XilunWu, https://github.com/yifuwang\n",
      ", Commit----------------------------------\n",
      "Revert \"[inductor] Enable subprocess-based parallel compile as the default (#126817)\"\n",
      "\n",
      "This reverts commit cf77e7dd9770caf65e898ac2ee82045aa0408e30.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/126817 on behalf of https://github.com/huydhn due to There are lots of flaky inductor failure showing up in trunk after this commit https://hud.pytorch.org/pytorch/pytorch/commit/cf77e7dd9770caf65e898ac2ee82045aa0408e30, so I am trying to revert this to see if this helps ([comment](https://github.com/pytorch/pytorch/pull/126817#issuecomment-2148143502))\n",
      ", Commit----------------------------------\n",
      "[dynamo] Bugfix for nn parameter construction (#127806)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127806\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785, #127802\n",
      ", Commit----------------------------------\n",
      "[dynamo] Unspec nn module when global backward hooks are present (#127802)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127802\n",
      "Approved by: https://github.com/jansel\n",
      "ghstack dependencies: #127785\n",
      ", Commit----------------------------------\n",
      "[dynamo] Tensorvariable - track grad with _grad field (#127785)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127785\n",
      "Approved by: https://github.com/jansel\n",
      ", Commit----------------------------------\n",
      "Using scalarType instead string in function _group_tensors_by_device_and_dtype. (#127869)\n",
      "\n",
      "Now torch.dtype can pass through pybind11, so modify function _group_tensors_by_device_and_dtype to using scalar type. And without convert torch.dtype and string in python and c++ side.\n",
      "@ezyang @bdhirsh\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127869\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Revert \"Retire torch.distributed.pipeline (#127354)\"\n",
      "\n",
      "This reverts commit b9c058c203ee38032594f898f27cd8404f113a63.\n",
      "\n",
      "Reverted https://github.com/pytorch/pytorch/pull/127354 on behalf of https://github.com/huydhn due to Sorry for reverting your change but the doc build failure looks legit https://hud.pytorch.org/pytorch/pytorch/commit/b9c058c203ee38032594f898f27cd8404f113a63 ([comment](https://github.com/pytorch/pytorch/pull/127354#issuecomment-2148133982))\n",
      ", Commit----------------------------------\n",
      "[CI] disable td for xpu ci test by default (#127611)\n",
      "\n",
      "Due to the xpu ci test has been enabled td by default, a lot of test cases (75%) have been skipped in CI tests. It caused some ci failures escaped from the ci tests, for example issue #127539. This PR depends on PR #127595 landed.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127611\n",
      "Approved by: https://github.com/etaf, https://github.com/atalman\n",
      ", Commit----------------------------------\n",
      "Enable UFMT on test/test_jit_fuser_te.py (#127759)\n",
      "\n",
      "Part of #123062\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127759\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "Use freshly traced jit-traced module to be used in export analysis (#127577)\n",
      "\n",
      "Summary: When we export already traced module, it seems to be modifying some global state causing the traced modules to fail to run. For now, we are only logging for test cases, so it is probs ok to trace fresh copy to be used in export for now.\n",
      "\n",
      "Test Plan: CI\n",
      "\n",
      "Differential Revision: D57983518\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127577\n",
      "Approved by: https://github.com/pianpwk\n",
      ", Commit----------------------------------\n",
      "[BE]: Update cudnn to 9.1.0.70 (#123475)\n",
      "\n",
      "cuDNN has managed to upload cu11 and cu12 wheels for ~~9.0.0.312~~ 9.1.0.70, so trying this out...\n",
      "\n",
      "CC @Skylion007 @malfet\n",
      "\n",
      "Co-authored-by: Wei Wang <weiwan@nvidia.com>\n",
      "Co-authored-by: atalman <atalman@fb.com>\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/123475\n",
      "Approved by: https://github.com/Skylion007, https://github.com/malfet, https://github.com/nWEIdia\n",
      ", Commit----------------------------------\n",
      "documentation for pattern_matcher.py (#127459)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127459\n",
      "Approved by: https://github.com/oulgen\n",
      "ghstack dependencies: #127457, #127458\n",
      ", Commit----------------------------------\n",
      "Add typing annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Turn on `mypy: disallow-untyped-defs` in pattern_matcher.py and fix the fallout.\n",
      "\n",
      "There are still a bunch of `type: ignore` annotations which should eventually be ironed out.\n",
      "\n",
      "In the processs found a bug: #127457\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127458\n",
      "Approved by: https://github.com/Skylion007\n",
      "ghstack dependencies: #127457\n",
      ", Commit----------------------------------\n",
      "fix post_grad pattern (#127457)\n",
      "\n",
      "The lowering pattern built by cuda_and_enabled_mixed_mm_and_not_int8() was using ListOf() incorrectly - ListOf() is meant to represent a single repeating pattern - but cuda_and_enabled_mixed_mm_and_not_int8() was passing two patterns - I think based on the comment it's trying to build a sequence which would be represented by an actual list, not ListOf().\n",
      "\n",
      "The behavior of the existing pattern would be to pass the second pattern as the `partial` parameter of `ListOf` which is meant to be a boolean - so it's almost certainly not what was intended.\n",
      "\n",
      "I tried changing it to be what I thought was the intended behavior but then the resnet152 test failed accuracy - so I'm just preserving the existing behavior with the correct parameter types.\n",
      "\n",
      "Found when adding annotations to pattern_matcher.py (#127458)\n",
      "\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127457\n",
      "Approved by: https://github.com/oulgen\n",
      ", Commit----------------------------------\n",
      "Concat namespaces and other fixes in torch/csrc/utils (#127833)\n",
      "\n",
      "It contains formatting and other minor fixes.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127833\n",
      "Approved by: https://github.com/ezyang\n",
      ", Commit----------------------------------\n",
      "[TORCH_FA2_flash_api] Update total_q to the reshaped query 0th dimension (#127524)\n",
      "\n",
      "There is a difference (&bug) between the TORCH_FA2_flash_api:**mha_varlen_fwd** and FA2_flash_api:**mha_varlen_fwd** at the query transposition (GQA) step.\n",
      "\n",
      "```\n",
      "at::Tensor temp_q = q;\n",
      "if (seqlenq_ngroups_swapped) {\n",
      "        temp_q = q.reshape( ...\n",
      " ...\n",
      "}\n",
      "const int total_q = q.sizes()[0];\n",
      "CHECK_SHAPE(temp_q, total_q, num_heads, head_size_og);\n",
      "```\n",
      "\n",
      "When doing query transposition we need to update total_q to the reshaped query 0th dimension, i.e:\n",
      "```\n",
      "const int total_q = temp_q.sizes()[0];\n",
      " ```\n",
      "\n",
      "In the original FA2_flash_api:**mha_varlen_fwd** they dont introduce a new variable temp_q but overwrite the q value directly.\n",
      "Pull Request resolved: https://github.com/pytorch/pytorch/pull/127524\n",
      "Approved by: https://github.com/drisspg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import git\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def describe_clusters(commit_contents, labels, num_clusters, commit_messages):\n",
    "    cluster_descriptions = []\n",
    "\n",
    "    for cluster_id in range(num_clusters):\n",
    "        cluster_contents = [commit_messages[i] for i in range(len(labels)) if labels[i] == cluster_id]\n",
    "        combined_text = \"\\n\\n\\n\".join(cluster_contents)\n",
    "        \n",
    "        description = \"Cluster {}: Commit messages: {}\".format(cluster_id, \", \".join(commit_messages))\n",
    "        cluster_descriptions.append(description)\n",
    "    \n",
    "    return cluster_descriptions\n",
    "\n",
    "# The rest of your script remains the same\n",
    "repo_path = '~/Downloads/pytorch'  # Update this path to your cloned repo\n",
    "repo = git.Repo(repo_path)\n",
    "\n",
    "# Extract commit messages and diffs\n",
    "commits = list(repo.iter_commits('main', max_count=20))\n",
    "commit_contents = []\n",
    "commit_messages = []\n",
    "\n",
    "for commit in commits:\n",
    "    commit_message = commit.message\n",
    "    commit_diff = commit.diff(create_patch=True)\n",
    "\n",
    "    diff_texts = []\n",
    "    for diff in commit_diff:\n",
    "        diff_texts.append(diff.diff.decode('utf-8'))\n",
    "\n",
    "    commit_diff_text = '\\n'.join(diff_texts)\n",
    "    content = f\"Commit Message:\\n{commit_message}\\n\\nCommit Diff:\\n{commit_diff_text}\"\n",
    "    commit_contents.append(content)\n",
    "    commit_messages.append(\"Commit----------------------------------\\n\"+commit_message)\n",
    "    \n",
    "# Check if commit contents are not empty\n",
    "if not commit_contents:\n",
    "    raise ValueError(\"No commit contents found in the repository\")\n",
    "\n",
    "# Vectorize commit contents using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(commit_contents)\n",
    "\n",
    "# Perform k-means clustering\n",
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42, n_init=10)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get cluster descriptions\n",
    "cluster_descriptions = describe_clusters(commit_contents, labels, num_clusters, commit_messages)\n",
    "\n",
    "# Print cluster descriptions\n",
    "for description in cluster_descriptions:\n",
    "    print(description)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
